---
title: Optimistic Bounds for Multi-output Learning
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/reeve20a/reeve20a.pdf
url: http://proceedings.mlr.press/v119/reeve20a.html
abstract: We investigate the challenge of multi-output learning, where the goal is
  to learn a vector-valued function based on a supervised data set. This includes
  a range of important problems in Machine Learning including multi-target regression,
  multi-class classification and multi-label classification. We begin our analysis
  by introducing the self-bounding Lipschitz condition for multi-output loss functions,
  which interpolates continuously between a classical Lipschitz condition and a multi-dimensional
  analogue of a smoothness condition. We then show that the self-bounding Lipschitz
  condition gives rise to optimistic bounds for multi-output learning, which attain
  the minimax optimal rate up to logarithmic factors. The proof exploits local Rademacher
  complexity combined with a powerful minoration inequality due to Srebro, Sridharan
  and Tewari. As an application we derive a state-of-the-art generalisation bound
  for multi-class gradient boosting.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: reeve20a
month: 0
tex_title: Optimistic Bounds for Multi-output Learning
firstpage: 8030
lastpage: 8040
page: 8030-8040
order: 8030
cycles: false
bibtex_author: Reeve, Henry and Kaban, Ata
author:
- given: Henry
  family: Reeve
- given: Ata
  family: Kaban
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/reeve20a/reeve20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
