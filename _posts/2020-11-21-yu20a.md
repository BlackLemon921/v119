---
title: 'Simultaneous Inference for Massive Data: Distributed Bootstrap'
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/yu20a/yu20a.pdf
url: http://proceedings.mlr.press/v119/yu20a.html
abstract: In this paper, we propose a bootstrap method applied to massive data processed
  distributedly in a large number of machines. This new method is computationally
  efficient in that we bootstrap on the master machine without over-resampling, typically
  required by existing methods (Kleiner et al., 2014; Sengupta et al., 2016), while
  provably achieving optimal statistical efficiency with minimal communication. Our
  method does not require repeatedly re-fitting the model but only applies multiplier
  bootstrap in the master machine on the gradients received from the worker machines.
  Simulations validate our theory.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: yu20a
month: 0
tex_title: 'Simultaneous Inference for Massive Data: Distributed Bootstrap'
firstpage: 10892
lastpage: 10901
page: 10892-10901
order: 10892
cycles: false
bibtex_author: Yu, Yang and Chao, Shih-Kang and Cheng, Guang
author:
- given: Yang
  family: Yu
- given: Shih-Kang
  family: Chao
- given: Guang
  family: Cheng
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/yu20a/yu20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
