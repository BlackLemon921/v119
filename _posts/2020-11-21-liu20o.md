---
title: Finding trainable sparse networks through Neural Tangent Transfer
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/liu20o/liu20o.pdf
url: http://proceedings.mlr.press/v119/liu20o.html
abstract: Deep neural networks have dramatically transformed machine learning, but
  their memory and energy demands are substantial. The requirements of real biological
  neural networks are rather modest in comparison, and one feature that might underlie
  this austerity is their sparse connectivity. In deep learning, trainable sparse
  networks that perform well on a specific task are usually constructed using label-dependent
  pruning criteria. In this article, we introduce Neural Tangent Transfer, a method
  that instead finds trainable sparse networks in a label-free manner. Specifically,
  we find sparse networks whose training dynamics, as characterized by the neural
  tangent kernel, mimic those of dense networks in function space. Finally, we evaluate
  our label-agnostic approach on several standard classification tasks and show that
  the resulting sparse networks achieve higher classification performance while converging
  faster.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu20o
month: 0
tex_title: Finding trainable sparse networks through Neural Tangent Transfer
firstpage: 6336
lastpage: 6347
page: 6336-6347
order: 6336
cycles: false
bibtex_author: Liu, Tianlin and Zenke, Friedemann
author:
- given: Tianlin
  family: Liu
- given: Friedemann
  family: Zenke
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/liu20o/liu20o-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
