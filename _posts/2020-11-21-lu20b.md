---
title: 'A Mean Field Analysis Of Deep ResNet And Beyond: Towards Provably Optimization
  Via Overparameterization From Depth'
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/lu20b/lu20b.pdf
url: http://proceedings.mlr.press/v119/lu20b.html
abstract: Training deep neural networks with stochastic gradient descent (SGD) can
  often achieve zero training loss on real-world tasks although the optimization landscape
  is known to be highly non-convex. To understand the success of SGD for training
  deep neural networks, this work presents a mean-field analysis of deep residual
  networks, based on a line of works which interpret the continuum limit of the deep
  residual network as an ordinary differential equation as the the network capacity
  tends to infinity. Specifically, we propose a \textbf{new continuum limit} of deep
  residual networks, which enjoys a good landscape in the sense that \textbf{every
  local minimizer is global}. This characterization enables us to derive the first
  global convergence result for multilayer neural networks in the mean-field regime.
  Furthermore, our proof does not rely on the convexity of the loss landscape, but
  instead, an assumption on the global minimizer should achieve zero loss which can
  be achieved when the model shares a universal approximation property. Key to our
  result is the observation that a deep residual network resembles a shallow network
  ensembleÂ \cite{veit2016residual}, \emph{i.e.} a two-layer network. We bound the
  difference between the shallow network and our ResNet model via the adjoint sensitivity
  method, which enables us to transfer previous mean-field analysis of two-layer networks
  to deep networks. Furthermore, we propose several novel training schemes based on
  our new continuous model, among which one new training procedure introduces the
  operation of switching the order of the residual blocks and results in strong empirical
  performance on benchmark datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lu20b
month: 0
tex_title: 'A Mean Field Analysis Of Deep {R}es{N}et And Beyond: Towards Provably
  Optimization Via Overparameterization From Depth'
firstpage: 6426
lastpage: 6436
page: 6426-6436
order: 6426
cycles: false
bibtex_author: Lu, Yiping and Ma, Chao and Lu, Yulong and Lu, Jianfeng and Ying, Lexing
author:
- given: Yiping
  family: Lu
- given: Chao
  family: Ma
- given: Yulong
  family: Lu
- given: Jianfeng
  family: Lu
- given: Lexing
  family: Ying
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/lu20b/lu20b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
