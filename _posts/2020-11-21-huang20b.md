---
title: From Importance Sampling to Doubly Robust Policy Gradient
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/huang20b/huang20b.pdf
url: http://proceedings.mlr.press/v119/huang20b.html
abstract: We show that on-policy policy gradient (PG) and its variance reduction variants
  can be derived by taking finite-difference of function evaluations supplied by estimators
  from the importance sampling (IS) family for off-policy evaluation (OPE). Starting
  from the doubly robust (DR) estimator (Jiang & Li, 2016), we provide a simple derivation
  of a very general and flexible form of PG, which subsumes the state-of-the-art variance
  reduction technique (Cheng et al., 2019) as its special case and immediately hints
  at further variance reduction opportunities overlooked by existing literature. We
  analyze the variance of the new DR-PG estimator, compare it to existing methods
  as well as the Cramer-Rao lower bound of policy gradient, and empirically show its
  effectiveness.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang20b
month: 0
tex_title: From Importance Sampling to Doubly Robust Policy Gradient
firstpage: 4434
lastpage: 4443
page: 4434-4443
order: 4434
cycles: false
bibtex_author: Huang, Jiawei and Jiang, Nan
author:
- given: Jiawei
  family: Huang
- given: Nan
  family: Jiang
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
software: https://github.com/Leonardo-H/DR-PG
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/huang20b/huang20b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
