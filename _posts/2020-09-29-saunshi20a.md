---
title: A Sample Complexity Separation between Non-Convex and Convex Meta-Learning
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/saunshi20a/saunshi20a.pdf
url: http://proceedings.mlr.press/v119/saunshi20a.html
abstract: One popular trend in meta-learning is to learn from many training tasks
  a common initialization that a gradient-based method can use to solve a new task
  with few samples. The theory of meta-learning is still in its early stages, with
  several recent learning-theoretic analyses of methods such as Reptile [Nichol et
  al., 2018] being for \emph{convex models}. This work shows that convex-case analysis
  might be insufficient to understand the success of meta-learning, and that even
  for non-convex models it is important to look inside the optimization black-box,
  specifically at properties of the optimization trajectory. We construct a simple
  meta-learning instance that captures the problem of one-dimensional subspace learning.
  For the convex formulation of linear regression on this instance, we show that the
  new task sample complexity of any \emph{initialization-based meta-learning} algorithm
  is $\Omega(d)$, where $d$ is the input dimension. In contrast, for the non-convex
  formulation of a two layer linear network on the same instance, we show that both
  Reptile and multi-task representation learning can have new task sample complexity
  of $O(1)$, demonstrating a separation from convex meta-learning. Crucially, analyses
  of the training dynamics of these methods reveal that they can meta-learn the correct
  subspace onto which the data should be projected.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: saunshi20a
month: 0
tex_title: A Sample Complexity Separation between Non-Convex and Convex Meta-Learning
firstpage: 8512
lastpage: 8521
page: 8512-8521
order: 8512
cycles: false
bibtex_author: Saunshi, Nikunj and Zhang, Yi and Khodak, Mikhail and Arora, Sanjeev
author:
- given: Nikunj
  family: Saunshi
- given: Yi
  family: Zhang
- given: Mikhail
  family: Khodak
- given: Sanjeev
  family: Arora
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/saunshi20a/saunshi20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
