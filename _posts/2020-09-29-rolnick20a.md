---
title: Reverse-engineering deep ReLU networks
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/rolnick20a/rolnick20a.pdf
url: http://proceedings.mlr.press/v119/rolnick20a.html
abstract: The output of a neural network depends on its architecture and weights in
  a highly nonlinear way, and it is often assumed that a networkâ€™s parameters cannot
  be recovered from its output. Here, we prove that, in fact, it is frequently possible
  to reconstruct the architecture, weights, and biases of a deep ReLU network by observing
  only its output. We leverage the fact that every ReLU network defines a piecewise
  linear function, where the boundaries between linear regions correspond to inputs
  for which some neuron in the network switches between inactive and active ReLU states.
  By dissecting the set of region boundaries into components associated with particular
  neurons, we show both theoretically and empirically that it is possible to recover
  the weights of neurons and their arrangement within the network, up to isomorphism.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rolnick20a
month: 0
tex_title: Reverse-engineering deep {R}e{LU} networks
firstpage: 8178
lastpage: 8187
page: 8178-8187
order: 8178
cycles: false
bibtex_author: Rolnick, David and Kording, Konrad
author:
- given: David
  family: Rolnick
- given: Konrad
  family: Kording
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/rolnick20a/rolnick20a-supp.pdf
- label: Other Files
  link: https://media.icml.cc/Conferences/ICML2020/v119/rolnick20a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
