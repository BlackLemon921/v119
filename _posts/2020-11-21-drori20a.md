---
title: The Complexity of Finding Stationary Points with Stochastic Gradient Descent
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/drori20a/drori20a.pdf
url: http://proceedings.mlr.press/v119/drori20a.html
abstract: We study the iteration complexity of stochastic gradient descent (SGD) for
  minimizing the gradient norm of smooth, possibly nonconvex functions. We provide
  several results, implying that the classical $\mathcal{O}(\epsilon^{-4})$ upper
  bound (for making the average gradient norm less than $\epsilon$) cannot be improved
  upon, unless a combination of additional assumptions is made. Notably, this holds
  even if we limit ourselves to convex quadratic functions. We also show that for
  nonconvex functions, the feasibility of minimizing gradients with SGD is surprisingly
  sensitive to the choice of optimality criteria.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: drori20a
month: 0
tex_title: The Complexity of Finding Stationary Points with Stochastic Gradient Descent
firstpage: 2658
lastpage: 2667
page: 2658-2667
order: 2658
cycles: false
bibtex_author: Drori, Yoel and Shamir, Ohad
author:
- given: Yoel
  family: Drori
- given: Ohad
  family: Shamir
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/drori20a/drori20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
