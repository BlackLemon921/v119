---
title: Learning Near Optimal Policies with Low Inherent Bellman Error
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/zanette20a/zanette20a.pdf
url: http://proceedings.mlr.press/v119/zanette20a.html
abstract: 'We study the exploration problem with approximate linear action-value functions
  in episodic reinforcement learning under the notion of low inherent Bellman error,
  a condition normally employed to show convergence of approximate value iteration.
  First we relate this condition to other common frameworks and show that it is strictly
  more general than the low rank (or linear) MDP assumption of prior work. Second
  we provide an algorithm with a high probability regret bound $\widetilde O(\sum_{t=1}^H
  d_t \sqrt{K} + \sum_{t=1}^H \sqrt{d_t} \IBE K)$ where $H$ is the horizon, $K$ is
  the number of episodes, $\IBE$ is the value if the inherent Bellman error and $d_t$
  is the feature dimension at timestep $t$. In addition, we show that the result is
  unimprovable beyond constants and logs by showing a matching lower bound. This has
  two important consequences: 1) it shows that exploration is possible using only
  \emph{batch assumptions} with an algorithm that achieves the optimal statistical
  rate for the setting we consider, which is more general than prior work on low-rank
  MDPs 2) the lack of closedness (measured by the inherent Bellman error) is only
  amplified by $\sqrt{d_t}$ despite working in the online setting. Finally, the algorithm
  reduces to the celebrated \textsc{LinUCB} when $H=1$ but with a different choice
  of the exploration parameter that allows handling misspecified contextual linear
  bandits. While computational tractability questions remain open for the MDP setting,
  this enriches the class of MDPs with a linear representation for the action-value
  function where statistically efficient reinforcement learning is possible.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zanette20a
month: 0
tex_title: Learning Near Optimal Policies with Low Inherent {B}ellman Error
firstpage: 10978
lastpage: 10989
page: 10978-10989
order: 10978
cycles: false
bibtex_author: Zanette, Andrea and Lazaric, Alessandro and Kochenderfer, Mykel and
  Brunskill, Emma
author:
- given: Andrea
  family: Zanette
- given: Alessandro
  family: Lazaric
- given: Mykel
  family: Kochenderfer
- given: Emma
  family: Brunskill
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/zanette20a/zanette20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
