---
title: 'Fractional Underdamped Langevin Dynamics: Retargeting SGD with Momentum under
  Heavy-Tailed Gradient Noise'
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/simsekli20a/simsekli20a.pdf
url: http://proceedings.mlr.press/v119/simsekli20a.html
abstract: Stochastic gradient descent with momentum (SGDm) is one of the most popular
  optimization algorithms in deep learning. While there is a rich theory of SGDm for
  convex problems, the theory is considerably less developed in the context of deep
  learning where the problem is non-convex and the gradient noise might exhibit a
  heavy-tailed behavior, as empirically observed in recent studies. In this study,
  we consider a \emph{continuous-time} variant of SGDm, known as the underdamped Langevin
  dynamics (ULD), and investigate its asymptotic properties under heavy-tailed perturbations.
  Supported by recent studies from statistical physics, we argue both theoretically
  and empirically that the heavy-tails of such perturbations can result in a bias
  even when the step-size is small, in the sense that \emph{the optima of stationary
  distribution} of the dynamics might not match \emph{the optima of the cost function
  to be optimized}. As a remedy, we develop a novel framework, which we coin as \emph{fractional}
  ULD (FULD), and prove that FULD targets the so-called Gibbs distribution, whose
  optima exactly match the optima of the original cost. We observe that the Euler
  discretization of FULD has noteworthy algorithmic similarities with \emph{natural
  gradient} methods and \emph{gradient clipping}, bringing a new perspective on understanding
  their role in deep learning. We support our theory with experiments conducted on
  a synthetic model and neural networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: simsekli20a
month: 0
tex_title: 'Fractional Underdamped {L}angevin Dynamics: Retargeting {SGD} with Momentum
  under Heavy-Tailed Gradient Noise'
firstpage: 8970
lastpage: 8980
page: 8970-8980
order: 8970
cycles: false
bibtex_author: Simsekli, Umut and Zhu, Lingjiong and Teh, Yee Whye and Gurbuzbalaban,
  Mert
author:
- given: Umut
  family: Simsekli
- given: Lingjiong
  family: Zhu
- given: Yee Whye
  family: Teh
- given: Mert
  family: Gurbuzbalaban
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
software: https://github.com/umutsimsekli/fuld
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/simsekli20a/simsekli20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
