---
title: Do RNN and LSTM have Long Memory?
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/zhao20c/zhao20c.pdf
url: http://proceedings.mlr.press/v119/zhao20c.html
abstract: The LSTM network was proposed to overcome the difficulty in learning long-term
  dependence, and has made significant advancements in applications. With its success
  and drawbacks in mind, this paper raises the question - do RNN and LSTM have long
  memory? We answer it partially by proving that RNN and LSTM do not have long memory
  from a statistical perspective. A new definition for long memory networks is further
  introduced, and it requires the model weights to decay at a polynomial rate. To
  verify our theory, we convert RNN and LSTM into long memory networks by making a
  minimal modification, and their superiority is illustrated in modeling long-term
  dependence of various datasets.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao20c
month: 0
tex_title: Do {RNN} and {LSTM} have Long Memory?
firstpage: 11365
lastpage: 11375
page: 11365-11375
order: 11365
cycles: false
bibtex_author: Zhao, Jingyu and Huang, Feiqing and Lv, Jia and Duan, Yanjie and Qin,
  Zhen and Li, Guodong and Tian, Guangjian
author:
- given: Jingyu
  family: Zhao
- given: Feiqing
  family: Huang
- given: Jia
  family: Lv
- given: Yanjie
  family: Duan
- given: Zhen
  family: Qin
- given: Guodong
  family: Li
- given: Guangjian
  family: Tian
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
software: https://github.com/huawei-noah/noah-research/tree/master/mRNN-mLSTM
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/zhao20c/zhao20c-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
