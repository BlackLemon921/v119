---
title: Training Neural Networks for and by Interpolation
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/berrada20a/berrada20a.pdf
url: http://proceedings.mlr.press/v119/berrada20a.html
abstract: 'In modern supervised learning, many deep neural networks are able to interpolate
  the data: the empirical loss can be driven to near zero on all samples simultaneously.
  In this work, we explicitly exploit this interpolation property for the design of
  a new optimization algorithm for deep learning, which we term Adaptive Learning-rates
  for Interpolation with Gradients (ALI-G). ALI-G retains the two main advantages
  of Stochastic Gradient Descent (SGD), which are (i) a low computational cost per
  iteration and (ii) good generalization performance in practice. At each iteration,
  ALI-G exploits the interpolation property to compute an adaptive learning-rate in
  closed form. In addition, ALI-G clips the learning-rate to a maximal value, which
  we prove to be helpful for non-convex problems. Crucially, in contrast to the learning-rate
  of SGD, the maximal learning-rate of ALI-G does not require a decay schedule. This
  makes ALI-G considerably easier to tune than SGD. We prove the convergence of ALI-G
  in various stochastic settings. Notably, we tackle the realistic case where the
  interpolation property is satisfied up to some tolerance. We also provide experiments
  on a variety of deep learning architectures and tasks: (i) learning a differentiable
  neural computer; (ii) training a wide residual network on the SVHN data set; (iii)
  training a Bi-LSTM on the SNLI data set; and (iv) training wide residual networks
  and densely connected networks on the CIFAR data sets. ALI-G produces state-of-the-art
  results among adaptive methods, and even yields comparable performance with SGD,
  which requires manually tuned learning-rate schedules. Furthermore, ALI-G is simple
  to implement in any standard deep learning framework and can be used as a drop-in
  replacement in existing code.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: berrada20a
month: 0
tex_title: Training Neural Networks for and by Interpolation
firstpage: 799
lastpage: 809
page: 799-809
order: 799
cycles: false
bibtex_author: Berrada, Leonard and Zisserman, Andrew and Kumar, M. Pawan
author:
- given: Leonard
  family: Berrada
- given: Andrew
  family: Zisserman
- given: M. Pawan
  family: Kumar
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
software: https://github.com/oval-group/ali-g
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/berrada20a/berrada20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
