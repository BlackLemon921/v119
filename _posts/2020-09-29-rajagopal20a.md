---
title: 'Multi-Precision Policy Enforced Training (MuPPET) : A Precision-Switching
  Strategy for Quantised Fixed-Point Training of CNNs'
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/rajagopal20a/rajagopal20a.pdf
url: http://proceedings.mlr.press/v119/rajagopal20a.html
abstract: Large-scale convolutional neural networks (CNNs) suffer from very long training
  times, spanning from hours to weeks, limiting the productivity and experimentation
  of deep learning practitioners. As networks grow in size and complexity, training
  time can be reduced through low-precision data representations and computations,
  however, in doing so the final accuracy suffers due to the problem of vanishing
  gradients. Existing state-of-the-art methods combat this issue by means of a mixed-precision
  approach utilising two different precision levels, FP32 (32-bit floating-point)
  and FP16/FP8 (16-/8-bit floating-point), leveraging the hardware support of recent
  GPU architectures for FP16 operations to obtain performance gains. This work pushes
  the boundary of quantised training by employing a multilevel optimisation approach
  that utilises multiple precisions including low-precision fixed-point representations
  resulting in a novel training strategy MuPPET; it combines the use of multiple number
  representation regimes together with a precision-switching mechanism that decides
  at run time the transition point between precision regimes. Overall, the proposed
  strategy tailors the training process to the hardware-level capabilities of the
  target hardware architecture and yields improvements in training time and energy
  efficiency compared to state-of-the-art approaches. Applying MuPPET on the training
  of AlexNet, ResNet18 and GoogLeNet on ImageNet (ILSVRC12) and targeting an NVIDIA
  Turing GPU, MuPPET achieves the same accuracy as standard full-precision training
  with training-time speedup of up to 1.84x and an average speedup of 1.58x across
  the networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: rajagopal20a
month: 0
tex_title: 'Multi-Precision Policy Enforced Training ({M}u{PPET}) : A Precision-Switching
  Strategy for Quantised Fixed-Point Training of {CNN}s'
firstpage: 7943
lastpage: 7952
page: 7943-7952
order: 7943
cycles: false
bibtex_author: Rajagopal, Aditya and Vink, Diederik and Venieris, Stylianos and Bouganis,
  Christos-Savvas
author:
- given: Aditya
  family: Rajagopal
- given: Diederik
  family: Vink
- given: Stylianos
  family: Venieris
- given: Christos-Savvas
  family: Bouganis
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
software: https://github.com/ICIdsl/pytorch_training.git
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/rajagopal20a/rajagopal20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
