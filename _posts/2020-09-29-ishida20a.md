---
title: Do We Need Zero Training Loss After Achieving Zero Training Error?
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/ishida20a/ishida20a.pdf
url: http://proceedings.mlr.press/v119/ishida20a.html
abstract: Overparameterized deep networks have the capacity to memorize training data
  with zero \emph{training error}. Even after memorization, the \emph{training loss}
  continues to approach zero, making the model overconfident and the test performance
  degraded. Since existing regularizers do not directly aim to avoid zero training
  loss, it is hard to tune their hyperparameters in order to maintain a fixed/preset
  level of training loss. We propose a direct solution called \emph{flooding} that
  intentionally prevents further reduction of the training loss when it reaches a
  reasonably small value, which we call the \emph{flood level}. Our approach makes
  the loss float around the flood level by doing mini-batched gradient descent as
  usual but gradient ascent if the training loss is below the flood level. This can
  be implemented with one line of code and is compatible with any stochastic optimizer
  and other regularizers. With flooding, the model will continue to “random walk”
  with the same non-zero training loss, and we expect it to drift into an area with
  a flat loss landscape that leads to better generalization. We experimentally show
  that flooding improves performance and, as a byproduct, induces a double descent
  curve of the test loss.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: ishida20a
month: 0
tex_title: Do We Need Zero Training Loss After Achieving Zero Training Error?
firstpage: 4604
lastpage: 4614
page: 4604-4614
order: 4604
cycles: false
bibtex_author: Ishida, Takashi and Yamane, Ikko and Sakai, Tomoya and Niu, Gang and
  Sugiyama, Masashi
author:
- given: Takashi
  family: Ishida
- given: Ikko
  family: Yamane
- given: Tomoya
  family: Sakai
- given: Gang
  family: Niu
- given: Masashi
  family: Sugiyama
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
software: https://github.com/takashiishida/flooding
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/ishida20a/ishida20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
