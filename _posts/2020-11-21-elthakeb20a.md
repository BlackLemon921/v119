---
title: 'Divide and Conquer: Leveraging Intermediate Feature Representations for Quantized
  Training of Neural Networks'
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/elthakeb20a/elthakeb20a.pdf
url: http://proceedings.mlr.press/v119/elthakeb20a.html
abstract: The deep layers of modern neural networks extract a rather rich set of features
  as an input propagates through the network, this paper sets out to harvest these
  rich intermediate representations for quantization with minimal accuracy loss while
  significantly reducing the memory footprint and compute intensity of the DNN. This
  paper utilizes knowledge distillation through teacher-student paradigm (Hinton et
  al., 2015) in a novel setting that exploits the feature extraction capability of
  DNNs for higher accuracy quantization. As such, our algorithm logically divides
  a pretrained full-precision DNN to multiple sections, each of which exposes intermediate
  features to train a team of students independently in the quantized domain and simply
  stitching them afterwards. This divide and conquer strategy, makes the training
  of each student section possible in isolation, speeding up training by enabling
  parallelization. Experiments on various DNNs (AlexNet, LeNet, MobileNet, ResNet-18,
  ResNet-20, SVHN and VGG-11) show that, this approach{—}called DCQ (Divide and Conquer
  Quantization){—}on average, improves the performance of a state-of-the-art quantized
  training technique, DoReFa-Net (Zhou et al., 2016) by 21.6% and 9.3% for binary
  and ternary quantization, respectively. Additionally, we show that incorporating
  DCQ to existing quantized training methods leads to improved accuracies as compared
  to previously reported by multiple state-of-the-art quantized training methods.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: elthakeb20a
month: 0
tex_title: 'Divide and Conquer: Leveraging Intermediate Feature Representations for
  Quantized Training of Neural Networks'
firstpage: 2880
lastpage: 2891
page: 2880-2891
order: 2880
cycles: false
bibtex_author: Elthakeb, Ahmed Taha and Pilligundla, Prannoy and Mireshghallah, Fatemeh
  and Cloninger, Alexander and Esmaeilzadeh, Hadi
author:
- given: Ahmed Taha
  family: Elthakeb
- given: Prannoy
  family: Pilligundla
- given: Fatemeh
  family: Mireshghallah
- given: Alexander
  family: Cloninger
- given: Hadi
  family: Esmaeilzadeh
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/elthakeb20a/elthakeb20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
