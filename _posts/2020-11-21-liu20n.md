---
title: Learning to Encode Position for Transformer with Continuous Dynamical Model
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/liu20n/liu20n.pdf
url: http://proceedings.mlr.press/v119/liu20n.html
abstract: 'We introduce a new way of learning to encode position information for non-recurrent
  models, such as Transformer models. Unlike RNN and LSTM, which contain inductive
  bias by loading the input tokens sequentially, non-recurrent models are less sensitive
  to position. The main reason is that position information among input units is not
  encoded inherently, i.e., they are permutation equivalent, this problem justifies
  why all of the existing models are accompanied by position encoding/embedding layer
  at the input. However, this solution has clear limitations: the sinusoidal position
  encoding is not flexible enough as it is manually designed and does not contain
  any learnable parameters, whereas the position embedding restricts the maximum length
  of input sequences. It is thus desirable to design a new position layer that contains
  learnable parameters to adjust to different datasets and different architectures.
  At the same time, we would also like it to extrapolate in accordance with the variable
  length of inputs. In our proposed solution, we borrow from the recent Neural ODE
  approach, which may be viewed as a versatile continuous version of a ResNet. This
  model is capable of modeling many kinds of dynamical systems. We model the evolution
  of encoded results along position index by such a dynamical system, thereby overcoming
  the above limitations of existing methods. We evaluate our new position layers on
  a variety of neural machine translation and language understanding tasks, the experimental
  results show consistent improvements over the baselines.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu20n
month: 0
tex_title: Learning to Encode Position for Transformer with Continuous Dynamical Model
firstpage: 6327
lastpage: 6335
page: 6327-6335
order: 6327
cycles: false
bibtex_author: Liu, Xuanqing and Yu, Hsiang-Fu and Dhillon, Inderjit and Hsieh, Cho-Jui
author:
- given: Xuanqing
  family: Liu
- given: Hsiang-Fu
  family: Yu
- given: Inderjit
  family: Dhillon
- given: Cho-Jui
  family: Hsieh
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/liu20n/liu20n-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
