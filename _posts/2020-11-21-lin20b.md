---
title: Extrapolation for Large-batch Training in Deep Learning
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/lin20b/lin20b.pdf
url: http://proceedings.mlr.press/v119/lin20b.html
abstract: Deep learning networks are typically trained by Stochastic Gradient Descent
  (SGD) methods that iteratively improve the model parameters by estimating a gradient
  on a very small fraction of the training data. A major roadblock faced when increasing
  the batch size to a substantial fraction of the training data for reducing training
  time is the persistent degradation in performance (generalization gap). To address
  this issue, recent work propose to add small perturbations to the model parameters
  when computing the stochastic gradients and report improved generalization performance
  due to smoothing effects. However, this approach is poorly understood; it requires
  often model-specific noise and fine-tuning. To alleviate these drawbacks, we propose
  to use instead computationally efficient extrapolation (extragradient) to stabilize
  the optimization trajectory while still benefiting from smoothing to avoid sharp
  minima. This principled approach is well grounded from an optimization perspective
  and we show that a host of variations can be covered in a unified framework that
  we propose. We prove the convergence of this novel scheme and rigorously evaluate
  its empirical performance on ResNet, LSTM, and Transformer. We demonstrate that
  in a variety of experiments the scheme allows scaling to much larger batch sizes
  than before whilst reaching or surpassing SOTA accuracy.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lin20b
month: 0
tex_title: Extrapolation for Large-batch Training in Deep Learning
firstpage: 6094
lastpage: 6104
page: 6094-6104
order: 6094
cycles: false
bibtex_author: Lin, Tao and Kong, Lingjing and Stich, Sebastian and Jaggi, Martin
author:
- given: Tao
  family: Lin
- given: Lingjing
  family: Kong
- given: Sebastian
  family: Stich
- given: Martin
  family: Jaggi
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/lin20b/lin20b-supp.pdf
- label: Other Files
  link: https://media.icml.cc/Conferences/ICML2020/v119/lin20b-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
