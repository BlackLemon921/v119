---
title: Tight Bounds on Minimax Regret under Logarithmic Loss via Self-Concordance
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/bilodeau20a/bilodeau20a.pdf
url: http://proceedings.mlr.press/v119/bilodeau20a.html
abstract: We consider the classical problem of sequential probability assignment under
  logarithmic loss while competing against an arbitrary, potentially nonparametric
  class of experts. We obtain tight bounds on the minimax regret via a new approach
  that exploits the self-concordance property of the logarithmic loss. We show that
  for any expert class with (sequential) metric entropy $\mathcal{O}(\gamma^{-p})$
  at scale $\gamma$, the minimax regret is $\mathcal{O}(n^{\frac{p}{p+1}})$, and that
  this rate cannot be improved without additional assumptions on the expert class
  under consideration. As an application of our techniques, we resolve the minimax
  regret for nonparametric Lipschitz classes of experts.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bilodeau20a
month: 0
tex_title: Tight Bounds on Minimax Regret under Logarithmic Loss via Self-Concordance
firstpage: 919
lastpage: 929
page: 919-929
order: 919
cycles: false
bibtex_author: Bilodeau, Blair and Foster, Dylan and Roy, Daniel
author:
- given: Blair
  family: Bilodeau
- given: Dylan
  family: Foster
- given: Daniel
  family: Roy
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/bilodeau20a/bilodeau20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
