---
title: Optimal Continual Learning has Perfect Memory and is NP-hard
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/knoblauch20a/knoblauch20a.pdf
url: http://proceedings.mlr.press/v119/knoblauch20a.html
abstract: Continual Learning (CL) algorithms incrementally learn a predictor or representation
  across multiple sequentially observed tasks. Designing CL algorithms that perform
  reliably and avoid so-called catastrophic forgetting has proven a persistent challenge.
  The current paper develops a theoretical approach that explains why. In particular,
  we derive the computational properties which CL algorithms would have to possess
  in order to avoid catastrophic forgetting. Our main finding is that such optimal
  CL algorithms generally solve an NP-hard problem and will require perfect memory
  to do so. The findings are of theoretical interest, but also explain the excellent
  performance of CL algorithms using experience replay, episodic memory and core sets
  relative to regularization-based approaches.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: knoblauch20a
month: 0
tex_title: Optimal Continual Learning has Perfect Memory and is {NP}-hard
firstpage: 5327
lastpage: 5337
page: 5327-5337
order: 5327
cycles: false
bibtex_author: Knoblauch, Jeremias and Husain, Hisham and Diethe, Tom
author:
- given: Jeremias
  family: Knoblauch
- given: Hisham
  family: Husain
- given: Tom
  family: Diethe
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/knoblauch20a/knoblauch20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
