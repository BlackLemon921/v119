---
title: Improving Transformer Optimization Through Better Initialization
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/huang20f/huang20f.pdf
url: http://proceedings.mlr.press/v119/huang20f.html
abstract: 'The Transformer architecture has achieved considerable success recently;
  the key component of the Transformer is the attention layer that enables the model
  to focus on important regions within an input sequence. Gradient optimization with
  attention layers can be notoriously difficult requiring tricks such as learning
  rate warmup to prevent divergence. As Transformer models are becoming larger and
  more expensive to train, recent research has focused on understanding and improving
  optimization in these architectures. In this work our contributions are two-fold:
  we first investigate and empirically validate the source of optimization problems
  in the encoder-decoder Transformer architecture; we then propose a new weight initialization
  scheme with theoretical justification, that enables training without warmup or layer
  normalization. Empirical results on public machine translation benchmarks show that
  our approach achieves leading accuracy, allowing to train deep Transformer models
  with 200 layers in both encoder and decoder (over 1000 attention/MLP blocks) without
  difficulty. Code for this work is available here:Â \url{https://github.com/layer6ai-labs/T-Fixup}.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang20f
month: 0
tex_title: Improving Transformer Optimization Through Better Initialization
firstpage: 4475
lastpage: 4483
page: 4475-4483
order: 4475
cycles: false
bibtex_author: Huang, Xiao Shi and Perez, Felipe and Ba, Jimmy and Volkovs, Maksims
author:
- given: Xiao Shi
  family: Huang
- given: Felipe
  family: Perez
- given: Jimmy
  family: Ba
- given: Maksims
  family: Volkovs
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
software: https://github.com/layer6ai-labs/T-Fixup
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/huang20f/huang20f-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
