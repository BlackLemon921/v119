---
title: Transformer Hawkes Process
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/zuo20a/zuo20a.pdf
url: http://proceedings.mlr.press/v119/zuo20a.html
abstract: Modern data acquisition routinely produce massive amounts of event sequence
  data in various domains, such as social media, healthcare, and financial markets.
  These data often exhibit complicated short-term and long-term temporal dependencies.
  However, most of the existing recurrent neural network based point process models
  fail to capture such dependencies, and yield unreliable prediction performance.
  To address this issue, we propose a Transformer Hawkes Process (THP) model, which
  leverages the self-attention mechanism to capture long-term dependencies and meanwhile
  enjoys computational efficiency. Numerical experiments on various datasets show
  that THP outperforms existing models in terms of both likelihood and event prediction
  accuracy by a notable margin. Moreover, THP is quite general and can incorporate
  additional structural knowledge. We provide a concrete example, where THP achieves
  improved prediction performance for learning multiple point processes when incorporating
  their relational information.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zuo20a
month: 0
tex_title: Transformer {H}awkes Process
firstpage: 11692
lastpage: 11702
page: 11692-11702
order: 11692
cycles: false
bibtex_author: Zuo, Simiao and Jiang, Haoming and Li, Zichong and Zhao, Tuo and Zha,
  Hongyuan
author:
- given: Simiao
  family: Zuo
- given: Haoming
  family: Jiang
- given: Zichong
  family: Li
- given: Tuo
  family: Zhao
- given: Hongyuan
  family: Zha
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
software: https://github.com/SimiaoZuo/Transformer-Hawkes-Process
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/zuo20a/zuo20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
