---
title: Convergence Rates of Variational Inference in Sparse Deep Learning
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/cherief-abdellatif20a/cherief-abdellatif20a.pdf
url: !binary |-
  aHR0cDovL3Byb2NlZWRpbmdzLm1sci5wcmVzcy92MTE5L2Now6lyaWVmYWJkZWxsYXRpZjIwYS5odG1s
abstract: Variational inference is becoming more and more popular for approximating
  intractable posterior distributions in Bayesian statistics and machine learning.
  Meanwhile, a few recent works have provided theoretical justification and new insights
  on deep neural networks for estimating smooth functions in usual settings such as
  nonparametric regression. In this paper, we show that variational inference for
  sparse deep learning retains precisely the same generalization properties than exact
  Bayesian inference. In particular, we show that a wise choice of the neural network
  architecture leads to near-minimax rates of convergence for Hölder smooth functions.
  Additionally, we show that the model selection framework over the architecture of
  the network via ELBO maximization does not overfit and adaptively achieves the optimal
  rate of convergence.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cherief-abdellatif20a
month: 0
tex_title: Convergence Rates of Variational Inference in Sparse Deep Learning
firstpage: 1831
lastpage: 1842
page: 1831-1842
order: 1831
cycles: false
bibtex_author: Ch{\'e}rief-Abdellatif, Badr-Eddine
author:
- given: Badr-Eddine
  family: Chérief-Abdellatif
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
extras:
- label: Other Files
  link: https://media.icml.cc/Conferences/ICML2020/v119/cheriefabdellatif20a-supp.zip
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
