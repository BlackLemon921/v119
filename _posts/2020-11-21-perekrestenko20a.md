---
title: Constructive Universal High-Dimensional Distribution Generation through Deep
  ReLU Networks
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/perekrestenko20a/perekrestenko20a.pdf
url: http://proceedings.mlr.press/v119/perekrestenko20a.html
abstract: We present an explicit deep neural network construction that transforms
  uniformly distributed one-dimensional noise into an arbitrarily close approximation
  of any two-dimensional Lipschitz-continuous target distribution. The key ingredient
  of our design is a generalization of the "space-filling" property of sawtooth functions
  discovered in (Bailey & Telgarsky, 2018). We elicit the importance of depth - in
  our neural network construction - in driving the Wasserstein distance between the
  target distribution and the approximation realized by the network to zero. An extension
  to output distributions of arbitrary dimension is outlined. Finally, we show that
  the proposed construction does not incur a cost - in terms of error measured in
  Wasserstein-distance - relative to generating $d$-dimensional target distributions
  from $d$ independent random variables.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: perekrestenko20a
month: 0
tex_title: Constructive Universal High-Dimensional Distribution Generation through
  Deep {R}e{LU} Networks
firstpage: 7610
lastpage: 7619
page: 7610-7619
order: 7610
cycles: false
bibtex_author: Perekrestenko, Dmytro and M{\"u}ller, Stephan and B{\"o}lcskei, Helmut
author:
- given: Dmytro
  family: Perekrestenko
- given: Stephan
  family: Müller
- given: Helmut
  family: Bölcskei
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
