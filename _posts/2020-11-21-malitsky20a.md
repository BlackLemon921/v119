---
title: Adaptive Gradient Descent without Descent
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/malitsky20a/malitsky20a.pdf
url: http://proceedings.mlr.press/v119/malitsky20a.html
abstract: 'We present a strikingly simple proof that two rules are sufficient to automate
  gradient descent: 1) don’t increase the stepsize too fast and 2) don’t overstep
  the local curvature. No need for functional values, no line search, no information
  about the function except for the gradients. By following these rules, you get a
  method adaptive to the local geometry, with convergence guarantees depending only
  on the smoothness in a neighborhood of a solution. Given that the problem is convex,
  our method converges even if the global smoothness constant is infinity. As an illustration,
  it can minimize arbitrary continuously twice-differentiable convex function. We
  examine its performance on a range of convex and nonconvex problems, including logistic
  regression and matrix factorization.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: malitsky20a
month: 0
tex_title: Adaptive Gradient Descent without Descent
firstpage: 6702
lastpage: 6712
page: 6702-6712
order: 6702
cycles: false
bibtex_author: Malitsky, Yura and Mishchenko, Konstantin
author:
- given: Yura
  family: Malitsky
- given: Konstantin
  family: Mishchenko
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
software: https://github.com/ymalitsky/adaptive_GD
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/malitsky20a/malitsky20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
