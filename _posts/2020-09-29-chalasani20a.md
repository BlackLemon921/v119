---
title: Concise Explanations of Neural Networks using Adversarial Training
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/chalasani20a/chalasani20a.pdf
url: http://proceedings.mlr.press/v119/chalasani20a.html
abstract: 'We show new connections between adversarial learning and explainability
  for deep neural networks (DNNs). One form of explanation of the output of a neural
  network model in terms of its input features, is a vector of feature-attributions,
  which can be generated by various techniques such as Integrated Gradients (IG),
  DeepSHAP, LIME, and CXPlain. Two desirable characteristics of an attribution-based
  explanation are: (1) \emph{sparseness}: the attributions of irrelevant or weakly
  relevant features should be negligible, thus resulting in \emph{concise} explanations
  in terms of the significant features, and (2) \emph{stability}: it should not vary
  significantly within a small local neighborhood of the input. Our first contribution
  is a theoretical exploration of how these two properties (when using IG-based attributions)
  are related to adversarial training, for a class of 1-layer networks (which includes
  logistic regression models for binary and multi-class classification); for these
  networks we show that (a) adversarial training using an $\ell_\infty$-bounded adversary
  produces models with sparse attribution vectors, and (b) natural model-training
  while encouraging stable explanations (via an extra term in the loss function),
  is equivalent to adversarial training. Our second contribution is an empirical verification
  of phenomenon (a), which we show, somewhat surprisingly, occurs \emph{not only in
  1-layer networks, but also DNNs trained on standard image datasets}, and extends
  beyond IG-based attributions, to those based on DeepSHAP: adversarial training with
  $\linf$-bounded perturbations yields significantly sparser attribution vectors,
  with little degradation in performance on natural test data, compared to natural
  training. Moreover, the sparseness of the attribution vectors is significantly better
  than that achievable via $\ell_1$-regularized natural training.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chalasani20a
month: 0
tex_title: Concise Explanations of Neural Networks using Adversarial Training
firstpage: 1383
lastpage: 1391
page: 1383-1391
order: 1383
cycles: false
bibtex_author: Chalasani, Prasad and Chen, Jiefeng and Chowdhury, Amrita Roy and Wu,
  Xi and Jha, Somesh
author:
- given: Prasad
  family: Chalasani
- given: Jiefeng
  family: Chen
- given: Amrita Roy
  family: Chowdhury
- given: Xi
  family: Wu
- given: Somesh
  family: Jha
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
software: https://github.com/jfc43/advex
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/chalasani20a/chalasani20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
