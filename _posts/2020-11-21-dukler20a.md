---
title: Optimization Theory for ReLU Neural Networks Trained with Normalization Layers
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/dukler20a/dukler20a.pdf
url: http://proceedings.mlr.press/v119/dukler20a.html
abstract: The current paradigm of deep neural networks has been successful in part
  due to the use of normalization layers. Normalization layers like Batch Normalization,
  Layer Normalization and Weight Normalization are ubiquitous in practice as they
  improve the generalization performance and training speed of neural networks significantly.
  Nonetheless, the vast majority of current deep learning theory and non-convex optimization
  literature focuses on the un-normalized setting. We bridge this gap by providing
  the first global convergence result for 2 layer non-linear neural networks with
  ReLU activations trained with a normalization layer, namely Weight Normalization.
  The analysis shows how the introduction of normalization layers changes the optimization
  landscape and in some settings enables faster convergence as compared with un-normalized
  neural networks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dukler20a
month: 0
tex_title: Optimization Theory for {R}e{LU} Neural Networks Trained with Normalization
  Layers
firstpage: 2751
lastpage: 2760
page: 2751-2760
order: 2751
cycles: false
bibtex_author: Dukler, Yonatan and Gu, Quanquan and Montufar, Guido
author:
- given: Yonatan
  family: Dukler
- given: Quanquan
  family: Gu
- given: Guido
  family: Montufar
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/dukler20a/dukler20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
