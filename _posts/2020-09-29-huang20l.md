---
title: Dynamics of Deep Neural Networks and Neural Tangent Hierarchy
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/huang20l/huang20l.pdf
url: http://proceedings.mlr.press/v119/huang20l.html
abstract: The evolution of a deep neural network trained by the gradient descent in
  the overparametrization regime can be described by its neural tangent kernel (NTK)
  \cite{jacot2018neural, du2018gradient1,du2018gradient2,arora2019fine}. It was observed
  \cite{arora2019exact} that there is a performance gap between the kernel regression
  using the limiting NTK and the deep neural networks. We study the dynamic of neural
  networks of finite width and derive an infinite hierarchy of differential equations,
  the neural tangent hierarchy (NTH). We prove that the NTH hierarchy truncated at
  the level $p\geq 2$ approximates the dynamic of the NTK up to arbitrary precision
  under certain conditions on the neural network width and the data set dimension.
  The assumptions needed for these approximations become weaker as $p$ increases.
  Finally, NTH can be viewed as higher order extensions of NTK. In particular, the
  NTH truncated at $p=2$ recovers the NTK dynamics.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: huang20l
month: 0
tex_title: Dynamics of Deep Neural Networks and Neural Tangent Hierarchy
firstpage: 4542
lastpage: 4551
page: 4542-4551
order: 4542
cycles: false
bibtex_author: Huang, Jiaoyang and Yau, Horng-Tzer
author:
- given: Jiaoyang
  family: Huang
- given: Horng-Tzer
  family: Yau
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
