---
title: Momentum Improves Normalized SGD
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/cutkosky20b/cutkosky20b.pdf
url: http://proceedings.mlr.press/v119/cutkosky20b.html
abstract: We provide an improved analysis of normalized SGD showing that adding momentum
  provably removes the need for large batch sizes on non-convex objectives. Then,
  we consider the case of objectives with bounded second derivative and show that
  in this case a small tweak to the momentum formula allows normalized SGD with momentum
  to find an $\epsilon$-critical point in $O(1/\epsilon^{3.5})$ iterations, matching
  the best-known rates without accruing any logarithmic factors or dependence on dimension.
  We provide an adaptive learning rate schedule that automatically improves convergence
  rates when the variance in the gradients is small. Finally, we show that our method
  is effective when employed on popular large scale tasks such as ResNet-50 and BERT
  pretraining, matching the performance of the disparate methods used to get state-of-the-art
  results on both tasks.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cutkosky20b
month: 0
tex_title: Momentum Improves Normalized {SGD}
firstpage: 2260
lastpage: 2268
page: 2260-2268
order: 2260
cycles: false
bibtex_author: Cutkosky, Ashok and Mehta, Harsh
author:
- given: Ashok
  family: Cutkosky
- given: Harsh
  family: Mehta
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
software: https://github.com/google-research/google-research/tree/master/nigt_optimizer
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/cutkosky20b/cutkosky20b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
