---
title: Linear Mode Connectivity and the Lottery Ticket Hypothesis
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/frankle20a/frankle20a.pdf
url: http://proceedings.mlr.press/v119/frankle20a.html
abstract: We study whether a neural network optimizes to the same, linearly connected
  minimum under different samples of SGD noise (e.g., random data order and augmentation).
  We find that standard vision models become stable to SGD noise in this way early
  in training. From then on, the outcome of optimization is determined to a linearly
  connected region. We use this technique to study iterative magnitude pruning (IMP),
  the procedure used by work on the lottery ticket hypothesis to identify subnetworks
  that could have trained in isolation to full accuracy. We find that these subnetworks
  only reach full accuracy when they are stable to SGD noise, which either occurs
  at initialization for small-scale settings (MNIST) or early in training for large-scale
  settings (ResNet-50 and Inception-v3 on ImageNet).
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: frankle20a
month: 0
tex_title: Linear Mode Connectivity and the Lottery Ticket Hypothesis
firstpage: 3259
lastpage: 3269
page: 3259-3269
order: 3259
cycles: false
bibtex_author: Frankle, Jonathan and Dziugaite, Gintare Karolina and Roy, Daniel and
  Carbin, Michael
author:
- given: Jonathan
  family: Frankle
- given: Gintare Karolina
  family: Dziugaite
- given: Daniel
  family: Roy
- given: Michael
  family: Carbin
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/frankle20a/frankle20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
