---
title: 'Moniqua: Modulo Quantized Communication in Decentralized SGD'
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/lu20a/lu20a.pdf
url: http://proceedings.mlr.press/v119/lu20a.html
abstract: Running Stochastic Gradient Descent (SGD) in a decentralized fashion has
  shown promising results. In this paper we propose Moniqua, a technique that allows
  decentralized SGD to use quantized communication. We prove in theory that Moniqua
  communicates a provably bounded number of bits per iteration, while converging at
  the same asymptotic rate as the original algorithm does with full-precision communication.
  Moniqua improves upon prior works in that it (1) requires zero additional memory,
  (2) works with 1-bit quantization, and (3) is applicable to a variety of decentralized
  algorithms. We demonstrate empirically that Moniqua converges faster with respect
  to wall clock time than other quantized decentralized algorithms. We also show that
  Moniqua is robust to very low bit-budgets, allowing $1$-bit-per-parameter communication
  without compromising validation accuracy when training ResNet20 and ResNet110 on
  CIFAR10.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: lu20a
month: 0
tex_title: 'Moniqua: Modulo Quantized Communication in Decentralized {SGD}'
firstpage: 6415
lastpage: 6425
page: 6415-6425
order: 6415
cycles: false
bibtex_author: Lu, Yucheng and De Sa, Christopher
author:
- given: Yucheng
  family: Lu
- given: Christopher
  family: De Sa
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/lu20a/lu20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
