---
title: 'Train Big, Then Compress: Rethinking Model Size for Efficient Training and
  Inference of Transformers'
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/li20m/li20m.pdf
url: http://proceedings.mlr.press/v119/li20m.html
abstract: 'Since hardware resources are limited, the objective of training deep learning
  models is typically to maximize accuracy subject to the time and memory constraints
  of training and inference. We study the impact of model size in this setting, focusing
  on Transformer models for NLP tasks that are limited by compute: self-supervised
  pretraining and high-resource machine translation. We first show that even though
  smaller Transformer models execute faster per iteration, wider and deeper models
  converge in significantly fewer steps. Moreover, this acceleration in convergence
  typically outpaces the additional computational overhead of using larger models.
  Therefore, the most compute-efficient training strategy is to counterintuitively
  train extremely large models but stop after a small number of iterations. This leads
  to an apparent trade-off between the training efficiency of large Transformer models
  and the inference efficiency of small Transformer models. However, we show that
  large models are more robust to compression techniques such as quantization and
  pruning than small models. Consequently, one can get the best of both worlds: heavily
  compressed, large models achieve higher accuracy than lightly compressed, small
  models.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: li20m
month: 0
tex_title: 'Train Big, Then Compress: Rethinking Model Size for Efficient Training
  and Inference of Transformers'
firstpage: 5958
lastpage: 5968
page: 5958-5968
order: 5958
cycles: false
bibtex_author: Li, Zhuohan and Wallace, Eric and Shen, Sheng and Lin, Kevin and Keutzer,
  Kurt and Klein, Dan and Gonzalez, Joey
author:
- given: Zhuohan
  family: Li
- given: Eric
  family: Wallace
- given: Sheng
  family: Shen
- given: Kevin
  family: Lin
- given: Kurt
  family: Keutzer
- given: Dan
  family: Klein
- given: Joey
  family: Gonzalez
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/li20m/li20m-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
