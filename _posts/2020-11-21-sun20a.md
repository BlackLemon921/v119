---
title: 'Improving the Sample and Communication Complexity for Decentralized Non-Convex
  Optimization: Joint Gradient Estimation and Tracking'
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/sun20a/sun20a.pdf
url: http://proceedings.mlr.press/v119/sun20a.html
abstract: "Many modern large-scale machine learning problems benefit from decentralized
  and stochastic optimization. Recent works have shown that utilizing both decentralized
  computing and local stochastic gradient estimates can outperform state-of-the-art
  centralized algorithms, in applications involving highly non-convex problems, such
  as training deep neural networks. \t In this work, we propose a decentralized stochastic
  algorithm to deal with certain smooth non-convex problems where there are $m$ nodes
  in the system, and each node has a large number of samples (denoted as $n$). Differently
  from the majority of the existing decentralized learning algorithms for either stochastic
  or finite-sum problems, our focus is given to \\emph{both} reducing the total communication
  rounds among the nodes, while accessing the minimum number of local data samples.
  In particular, we propose an algorithm named D-GET (decentralized gradient estimation
  and tracking), which jointly performs decentralized gradient estimation (which estimates
  the local gradient using a subset of local samples) \\emph{and} gradient tracking
  (which tracks the global full gradient using local estimates). We show that to achieve
  certain $\\epsilon$ stationary solution of the deterministic finite sum problem,
  the proposed algorithm achieves an $\\mathcal{O}(mn^{1/2}\\epsilon^{-1})$ sample
  complexity and an $\\mathcal{O}(\\epsilon^{-1})$ communication complexity. These
  bounds significantly improve upon the best existing bounds of $\\mathcal{O}(mn\\epsilon^{-1})$
  and $\\mathcal{O}(\\epsilon^{-1})$, respectively. Similarly, for online problems,
  the proposed method achieves an $\\mathcal{O}(m \\epsilon^{-3/2})$ sample complexity
  and an $\\mathcal{O}(\\epsilon^{-1})$ communication complexity."
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: sun20a
month: 0
tex_title: 'Improving the Sample and Communication Complexity for Decentralized Non-Convex
  Optimization: Joint Gradient Estimation and Tracking'
firstpage: 9217
lastpage: 9228
page: 9217-9228
order: 9217
cycles: false
bibtex_author: Sun, Haoran and Lu, Songtao and Hong, Mingyi
author:
- given: Haoran
  family: Sun
- given: Songtao
  family: Lu
- given: Mingyi
  family: Hong
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/sun20a/sun20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
