---
title: Inducing and Exploiting Activation Sparsity for Fast Inference on Deep Neural
  Networks
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/kurtz20a/kurtz20a.pdf
url: http://proceedings.mlr.press/v119/kurtz20a.html
abstract: 'Optimizing convolutional neural networks for fast inference has recently
  become an extremely active area of research. One of the go-to solutions in this
  context is weight pruning, which aims to reduce computational and memory footprint
  by removing large subsets of the connections in a neural network. Surprisingly,
  much less attention has been given to exploiting sparsity in the activation maps,
  which tend to be naturally sparse in many settings thanks to the structure of rectified
  linear (ReLU) activation functions. In this paper, we present an in-depth analysis
  of methods for maximizing the sparsity of the activations in a trained neural network,
  and show that, when coupled with an efficient sparse-input convolution algorithm,
  we can leverage this sparsity for significant performance gains. To induce highly
  sparse activation maps without accuracy loss, we introduce a new regularization
  technique, coupled with a new threshold-based sparsification method based on a parameterized
  activation function called Forced-Activation-Threshold Rectified Linear Unit (FATReLU).
  We examine the impact of our methods on popular image classification models, showing
  that most architectures can adapt to significantly sparser activation maps without
  any accuracy loss. Our second contribution is showing that these these compression
  gains can be translated into inference speedups: we provide a new algorithm to enable
  fast convolution operations over networks with sparse activations, and show that
  it can enable significant speedups for end-to-end inference on a range of popular
  models on the large-scale ImageNet image classification task on modern Intel CPUs,
  with little or no retraining cost.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kurtz20a
month: 0
tex_title: Inducing and Exploiting Activation Sparsity for Fast Inference on Deep
  Neural Networks
firstpage: 5533
lastpage: 5543
page: 5533-5543
order: 5533
cycles: false
bibtex_author: Kurtz, Mark and Kopinsky, Justin and Gelashvili, Rati and Matveev,
  Alexander and Carr, John and Goin, Michael and Leiserson, William and Moore, Sage
  and Shavit, Nir and Alistarh, Dan
author:
- given: Mark
  family: Kurtz
- given: Justin
  family: Kopinsky
- given: Rati
  family: Gelashvili
- given: Alexander
  family: Matveev
- given: John
  family: Carr
- given: Michael
  family: Goin
- given: William
  family: Leiserson
- given: Sage
  family: Moore
- given: Nir
  family: Shavit
- given: Dan
  family: Alistarh
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
