---
title: Being Bayesian about Categorical Probability
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/joo20a/joo20a.pdf
url: http://proceedings.mlr.press/v119/joo20a.html
abstract: Neural networks utilize the softmax as a building block in classification
  tasks, which contains an overconfidence problem and lacks an uncertainty representation
  ability. As a Bayesian alternative to the softmax, we consider a random variable
  of a categorical probability over class labels. In this framework, the prior distribution
  explicitly models the presumed noise inherent in the observed label, which provides
  consistent gains in generalization performance in multiple challenging tasks. The
  proposed method inherits advantages of Bayesian approaches that achieve better uncertainty
  estimation and model calibration. Our method can be implemented as a plug-and-play
  loss function with negligible computational overhead compared to the softmax with
  the cross-entropy loss function.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: joo20a
month: 0
tex_title: Being {B}ayesian about Categorical Probability
firstpage: 4950
lastpage: 4961
page: 4950-4961
order: 4950
cycles: false
bibtex_author: Joo, Taejong and Chung, Uijung and Seo, Min-Gwan
author:
- given: Taejong
  family: Joo
- given: Uijung
  family: Chung
- given: Min-Gwan
  family: Seo
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
