---
title: Generative Pretraining From Pixels
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/chen20s/chen20s.pdf
url: http://proceedings.mlr.press/v119/chen20s.html
abstract: Inspired by progress in unsupervised representation learning for natural
  language, we examine whether similar models can learn useful representations for
  images. We train a sequence Transformer to auto-regressively predict pixels, without
  incorporating knowledge of the 2D input structure. Despite training on low-resolution
  ImageNet without labels, we find that a GPT-2 scale model learns strong image representations
  as measured by linear probing, fine-tuning, and low-data classification. On CIFAR-10,
  we achieve 96.3% accuracy with a linear probe, outperforming a supervised Wide ResNet,
  and 99.0% accuracy with full fine-tuning, matching the top supervised pre-trained
  models. We are also competitive with self-supervised benchmarks on ImageNet when
  substituting pixels for a VQVAE encoding, achieving 69.0% top-1 accuracy on a linear
  probe of our features.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen20s
month: 0
tex_title: Generative Pretraining From Pixels
firstpage: 1691
lastpage: 1703
page: 1691-1703
order: 1691
cycles: false
bibtex_author: Chen, Mark and Radford, Alec and Child, Rewon and Wu, Jeffrey and Jun,
  Heewoo and Luan, David and Sutskever, Ilya
author:
- given: Mark
  family: Chen
- given: Alec
  family: Radford
- given: Rewon
  family: Child
- given: Jeffrey
  family: Wu
- given: Heewoo
  family: Jun
- given: David
  family: Luan
- given: Ilya
  family: Sutskever
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
