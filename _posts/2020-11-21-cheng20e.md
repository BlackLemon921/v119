---
title: Stochastic Gradient and Langevin Processes
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/cheng20e/cheng20e.pdf
url: http://proceedings.mlr.press/v119/cheng20e.html
abstract: We prove quantitative convergence rates at which discrete Langevin-like
  processes converge to the invariant distribution of a related stochastic differential
  equation. We study the setup where the additive noise can be non-Gaussian and state-dependent
  and the potential function can be non-convex. We show that the key properties of
  these processes depend on the potential function and the second moment of the additive
  noise. We apply our theoretical findings to studying the convergence of Stochastic
  Gradient Descent (SGD) for non-convex problems and corroborate them with experiments
  using SGD to train deep neural networks on the CIFAR-10 dataset.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cheng20e
month: 0
tex_title: Stochastic Gradient and {L}angevin Processes
firstpage: 1810
lastpage: 1819
page: 1810-1819
order: 1810
cycles: false
bibtex_author: Cheng, Xiang and Yin, Dong and Bartlett, Peter and Jordan, Michael
author:
- given: Xiang
  family: Cheng
- given: Dong
  family: Yin
- given: Peter
  family: Bartlett
- given: Michael
  family: Jordan
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/cheng20e/cheng20e-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
