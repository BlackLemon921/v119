---
title: On Layer Normalization in the Transformer Architecture
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/xiong20b/xiong20b.pdf
url: http://proceedings.mlr.press/v119/xiong20b.html
abstract: The Transformer is widely used in natural language processing tasks. To
  train a Transformer however, one usually needs a carefully designed learning rate
  warm-up stage, which is shown to be crucial to the final performance but will slow
  down the optimization and bring more hyper-parameter tunings. In this paper, we
  first study theoretically why the learning rate warm-up stage is essential and show
  that the location of layer normalization matters. Specifically, we prove with mean
  field theory that at initialization, for the original-designed Post-LN Transformer,
  which places the layer normalization between the residual blocks, the expected gradients
  of the parameters near the output layer are large. Therefore, using a large learning
  rate on those gradients makes the training unstable. The warm-up stage is practically
  helpful for avoiding this problem. On the other hand, our theory also shows that
  if the layer normalization is put inside the residual blocks (recently proposed
  as Pre-LN Transformer), the gradients are well-behaved at initialization. This motivates
  us to remove the warm-up stage for the training of Pre-LN Transformers. We show
  in our experiments that Pre-LN Transformers without the warm-up stage can reach
  comparable results with baselines while requiring significantly less training time
  and hyper-parameter tuning on a wide range of applications.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: xiong20b
month: 0
tex_title: On Layer Normalization in the Transformer Architecture
firstpage: 10524
lastpage: 10533
page: 10524-10533
order: 10524
cycles: false
bibtex_author: Xiong, Ruibin and Yang, Yunchang and He, Di and Zheng, Kai and Zheng,
  Shuxin and Xing, Chen and Zhang, Huishuai and Lan, Yanyan and Wang, Liwei and Liu,
  Tieyan
author:
- given: Ruibin
  family: Xiong
- given: Yunchang
  family: Yang
- given: Di
  family: He
- given: Kai
  family: Zheng
- given: Shuxin
  family: Zheng
- given: Chen
  family: Xing
- given: Huishuai
  family: Zhang
- given: Yanyan
  family: Lan
- given: Liwei
  family: Wang
- given: Tieyan
  family: Liu
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/xiong20b/xiong20b-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
