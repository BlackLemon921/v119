---
title: Continuous-time Lower Bounds for Gradient-based Algorithms
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/muehlebach20a/muehlebach20a.pdf
url: http://proceedings.mlr.press/v119/muehlebach20a.html
abstract: This article derives lower bounds on the convergence rate of continuous-time
  gradient-based optimization algorithms. The algorithms are subjected to a time-normalization
  constraint that avoids a reparametrization of time in order to make the discussion
  of continuous-time convergence rates meaningful. We reduce the multi-dimensional
  problem to a single dimension, recover well-known lower bounds from the discrete-time
  setting, and provide insight into why these lower bounds occur. We present algorithms
  that achieve the proposed lower bounds, even when the function class under consideration
  includes certain nonconvex functions.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: muehlebach20a
month: 0
tex_title: Continuous-time Lower Bounds for Gradient-based Algorithms
firstpage: 7088
lastpage: 7096
page: 7088-7096
order: 7088
cycles: false
bibtex_author: Muehlebach, Michael and Jordan, Michael
author:
- given: Michael
  family: Muehlebach
- given: Michael
  family: Jordan
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/muehlebach20a/muehlebach20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
