---
title: Student Specialization in Deep Rectified Networks With Finite Width and Input
  Dimension
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/tian20a/tian20a.pdf
url: http://proceedings.mlr.press/v119/tian20a.html
abstract: 'We consider a deep ReLU / Leaky ReLU student network trained from the output
  of a fixed teacher network of the same depth, with Stochastic Gradient Descent (SGD).
  The student network is \emph{over-realized}: at each layer $l$, the number $n_l$
  of student nodes is more than that ($m_l$) of teacher. Under mild conditions on
  dataset and teacher network, we prove that when the gradient is small at every data
  sample, each teacher node is \emph{specialized} by at least one student node \emph{at
  the lowest layer}. For two-layer network, such specialization can be achieved by
  training on any dataset of \emph{polynomial} size $\mathcal{O}( K^{5/2} d^3 \epsilon^{-1})$.
  until the gradient magnitude drops to $\mathcal{O}(\epsilon/K^{3/2}\sqrt{d})$. Here
  $d$ is the input dimension, $K = m_1 + n_1$ is the total number of neurons in the
  lowest layer of teacher and student. Note that we require a specific form of data
  augmentation and the sample complexity includes the additional data generated from
  augmentation. To our best knowledge, we are the first to give polynomial sample
  complexity for student specialization of training two-layer (Leaky) ReLU networks
  with finite depth and width in teacher-student setting, and finite complexity for
  the lowest layer specialization in multi-layer case, without parametric assumption
  of the input (like Gaussian). Our theory suggests that teacher nodes with large
  fan-out weights get specialized first when the gradient is still large, while others
  are specialized with small gradient, which suggests inductive bias in training.
  This shapes the stage of training as empirically observed in multiple previous works.
  Experiments on synthetic and CIFAR10 verify our findings. The code is released in
  \url{https://github.com/facebookresearch/luckmatters}.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tian20a
month: 0
tex_title: Student Specialization in Deep Rectified Networks With Finite Width and
  Input Dimension
firstpage: 9470
lastpage: 9480
page: 9470-9480
order: 9470
cycles: false
bibtex_author: Tian, Yuandong
author:
- given: Yuandong
  family: Tian
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/tian20a/tian20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
