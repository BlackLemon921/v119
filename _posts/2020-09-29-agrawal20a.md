---
title: Optimal Bounds between f-Divergences and Integral Probability Metrics
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/agrawal20a/agrawal20a.pdf
url: http://proceedings.mlr.press/v119/agrawal20a.html
abstract: The families of f-divergences (e.g. the Kullback-Leibler divergence) and
  Integral Probability Metrics (e.g. total variation distance or maximum mean discrepancies)
  are commonly used in optimization and estimation. In this work, we systematically
  study the relationship between these two families from the perspective of convex
  duality. Starting from a tight variational representation of the f-divergence, we
  derive a generalization of the moment generating function, which we show exactly
  characterizes the best lower bound of the f-divergence as a function of a given
  IPM. Using this characterization, we obtain new bounds on IPMs defined by classes
  of unbounded functions, while also recovering in a unified manner well-known results
  for bounded and subgaussian functions (e.g. Pinsker’s inequality and Hoeffding’s
  lemma).
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: agrawal20a
month: 0
tex_title: Optimal Bounds between f-Divergences and Integral Probability Metrics
firstpage: 115
lastpage: 124
page: 115-124
order: 115
cycles: false
bibtex_author: Agrawal, Rohit and Horel, Thibaut
author:
- given: Rohit
  family: Agrawal
- given: Thibaut
  family: Horel
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
