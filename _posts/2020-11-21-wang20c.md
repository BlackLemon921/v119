---
title: Towards Accurate Post-training Network Quantization via Bit-Split and Stitching
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/wang20c/wang20c.pdf
url: http://proceedings.mlr.press/v119/wang20c.html
abstract: Network quantization is essential for deploying deep models to IoT devices
  due to its high efficiency. Most existing quantization approaches rely on the full
  training datasets and the time-consuming fine-tuning to retain accuracy. Post-training
  quantization does not have these problems, however, it has mainly been shown effective
  for 8-bit quantization due to the simple optimization strategy. In this paper, we
  propose a Bit-Split and Stitching framework (Bit-split) for lower-bit post-training
  quantization with minimal accuracy degradation. The proposed framework is validated
  on a variety of computer vision tasks, including image classification, object detection,
  instance segmentation, with various network architectures. Specifically, Bit-split
  can achieve near-original model performance even when quantizing FP32 models to
  INT3 without fine-tuning.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang20c
month: 0
tex_title: Towards Accurate Post-training Network Quantization via Bit-Split and Stitching
firstpage: 9847
lastpage: 9856
page: 9847-9856
order: 9847
cycles: false
bibtex_author: Wang, Peisong and Chen, Qiang and He, Xiangyu and Cheng, Jian
author:
- given: Peisong
  family: Wang
- given: Qiang
  family: Chen
- given: Xiangyu
  family: He
- given: Jian
  family: Cheng
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
