---
title: 'PowerNorm: Rethinking Batch Normalization in Transformers'
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/shen20e/shen20e.pdf
url: http://proceedings.mlr.press/v119/shen20e.html
abstract: The standard normalization method for neural network (NN) models used in
  Natural Language Processing (NLP) is layer normalization (LN).This is different
  than batch normalization (BN), which is widely-adopted in Computer Vision. The preferred
  use of LN in NLP is principally due to the empirical observation that a (naive/vanilla)
  use of BN leads to significant performance degradation for NLP tasks; however, a
  thorough understanding of the underlying reasons for this is not always evident.
  In this paper, we perform a systematic study of NLP transformer models to understand
  why BN has a poor performance, as compared to LN. We find that the statistics of
  NLP data across the batch dimension exhibit large fluctuations throughout training.
  This results in instability, if BN is naively implemented. To address this, we propose
  Power Normalization (PN), a novel normalization scheme that resolves this issue
  by (i) relaxing zero-mean normalization in BN, (ii) incorporating a running quadratic
  mean instead of per batch statistics to stabilize fluctuations, and (iii) using
  an approximate backpropagation for incorporating the running statistics in the forward
  pass. We show theoretically, under mild assumptions, that PN leads to a smaller
  Lipschitz constant for the loss, compared with BN. Furthermore, we prove that the
  approximate backpropagation scheme leads to bounded gradients. We extensively test
  PN for transformers on a range of NLP tasks, and we show that it significantly outperforms
  both LN and BN. In particular, PN outperforms LN by 0.4/0.6 BLEU on IWSLT14/WMT14
  and 5.6/3.0 PPL on PTB/WikiText-103. We make our code publicly available at https://github.com/sIncerass/powernorm.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: shen20e
month: 0
tex_title: "{P}ower{N}orm: Rethinking Batch Normalization in Transformers"
firstpage: 8741
lastpage: 8751
page: 8741-8751
order: 8741
cycles: false
bibtex_author: Shen, Sheng and Yao, Zhewei and Gholami, Amir and Mahoney, Michael
  and Keutzer, Kurt
author:
- given: Sheng
  family: Shen
- given: Zhewei
  family: Yao
- given: Amir
  family: Gholami
- given: Michael
  family: Mahoney
- given: Kurt
  family: Keutzer
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/shen20e/shen20e-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
