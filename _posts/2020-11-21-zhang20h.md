---
title: Approximation Capabilities of Neural ODEs and Invertible Residual Networks
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/zhang20h/zhang20h.pdf
url: http://proceedings.mlr.press/v119/zhang20h.html
abstract: 'Recent interest in invertible models and normalizing flows has resulted
  in new architectures that ensure invertibility of the network model. Neural ODEs
  and i-ResNets are two recent techniques for constructing models that are invertible,
  but it is unclear if they can be used to approximate any continuous invertible mapping.
  Here, we show that out of the box, both of these architectures are limited in their
  approximation capabilities. We then show how to overcome this limitation: we prove
  that any homeomorphism on a $p$-dimensional Euclidean space can be approximated
  by a Neural ODE or an i-ResNet operating on a $2p$-dimensional Euclidean space.
  We conclude by showing that capping a Neural ODE or an i-ResNet with a single linear
  layer is sufficient to turn the model into a universal approximator for non-invertible
  continuous functions.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang20h
month: 0
tex_title: Approximation Capabilities of Neural {ODE}s and Invertible Residual Networks
firstpage: 11086
lastpage: 11095
page: 11086-11095
order: 11086
cycles: false
bibtex_author: Zhang, Han and Gao, Xi and Unterman, Jacob and Arodz, Tom
author:
- given: Han
  family: Zhang
- given: Xi
  family: Gao
- given: Jacob
  family: Unterman
- given: Tom
  family: Arodz
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
