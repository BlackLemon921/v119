---
title: Non-autoregressive Machine Translation with Disentangled Context Transformer
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/kasai20a/kasai20a.pdf
url: http://proceedings.mlr.press/v119/kasai20a.html
abstract: State-of-the-art neural machine translation models generate a translation
  from left to right and every step is conditioned on the previously generated tokens.
  The sequential nature of this generation process causes fundamental latency in inference
  since we cannot generate multiple tokens in each sentence in parallel. We propose
  an attention-masking based model, called Disentangled Context (DisCo) transformer,
  that simultaneously generates all tokens given different contexts. The DisCo transformer
  is trained to predict every output token given an arbitrary subset of the other
  reference tokens. We also develop the parallel easy-first inference algorithm, which
  iteratively refines every token in parallel and reduces the number of required iterations.
  Our extensive experiments on 7 translation directions with varying data sizes demonstrate
  that our model achieves competitive, if not better, performance compared to the
  state of the art in non-autoregressive machine translation while significantly reducing
  decoding time on average.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: kasai20a
month: 0
tex_title: Non-autoregressive Machine Translation with Disentangled Context Transformer
firstpage: 5144
lastpage: 5155
page: 5144-5155
order: 5144
cycles: false
bibtex_author: Kasai, Jungo and Cross, James and Ghazvininejad, Marjan and Gu, Jiatao
author:
- given: Jungo
  family: Kasai
- given: James
  family: Cross
- given: Marjan
  family: Ghazvininejad
- given: Jiatao
  family: Gu
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
