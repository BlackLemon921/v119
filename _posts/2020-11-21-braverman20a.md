---
title: Calibration, Entropy Rates, and Memory in Language Models
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/braverman20a/braverman20a.pdf
url: http://proceedings.mlr.press/v119/braverman20a.html
abstract: 'Building accurate language models that capture meaningful long-term dependencies
  is a core challenge in natural language processing. Towards this end, we present
  a calibration-based approach to measure long-term discrepancies between a generative
  sequence model and the true distribution, and use these discrepancies to improve
  the model. Empirically, we show that state-of-the-art language models, including
  LSTMs and Transformers, are miscalibrated: the entropy rates of their generations
  drift dramatically upward over time. We then provide provable methods to mitigate
  this phenomenon. Furthermore, we show how this calibration-based approach can also
  be used to measure the amount of memory that language models use for prediction.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: braverman20a
month: 0
tex_title: Calibration, Entropy Rates, and Memory in Language Models
firstpage: 1089
lastpage: 1099
page: 1089-1099
order: 1089
cycles: false
bibtex_author: Braverman, Mark and Chen, Xinyi and Kakade, Sham and Narasimhan, Karthik
  and Zhang, Cyril and Zhang, Yi
author:
- given: Mark
  family: Braverman
- given: Xinyi
  family: Chen
- given: Sham
  family: Kakade
- given: Karthik
  family: Narasimhan
- given: Cyril
  family: Zhang
- given: Yi
  family: Zhang
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/braverman20a/braverman20a-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
