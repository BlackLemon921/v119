---
title: Spread Divergence
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/zhang20j/zhang20j.pdf
url: http://proceedings.mlr.press/v119/zhang20j.html
abstract: For distributions $\mathbb{P}$ and $\mathbb{Q}$ with different supports
  or undefined densities, the divergence $\textrm{D}(\mathbb{P}||\mathbb{Q})$ may
  not exist. We define a Spread Divergence $\tilde{\textrm{D}}(\mathbb{P}||\mathbb{Q})$
  on modified $\mathbb{P}$ and $\mathbb{Q}$ and describe sufficient conditions for
  the existence of such a divergence. We demonstrate how to maximize the discriminatory
  power of a given divergence by parameterizing and learning the spread. We also give
  examples of using a Spread Divergence to train implicit generative models, including
  linear models (Independent Components Analysis) and non-linear models (Deep Generative
  Networks).
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang20j
month: 0
tex_title: Spread Divergence
firstpage: 11106
lastpage: 11116
page: 11106-11116
order: 11106
cycles: false
bibtex_author: Zhang, Mingtian and Hayes, Peter and Bird, Thomas and Habib, Raza and
  Barber, David
author:
- given: Mingtian
  family: Zhang
- given: Peter
  family: Hayes
- given: Thomas
  family: Bird
- given: Raza
  family: Habib
- given: David
  family: Barber
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/zhang20j/zhang20j-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
