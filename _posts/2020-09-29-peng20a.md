---
title: Non-Autoregressive Neural Text-to-Speech
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/peng20a/peng20a.pdf
url: http://proceedings.mlr.press/v119/peng20a.html
abstract: In this work, we propose ParaNet, a non-autoregressive seq2seq model that
  converts text to spectrogram. It is fully convolutional and brings 46.7 times speed-up
  over the lightweight Deep Voice 3 at synthesis, while obtaining reasonably good
  speech quality. ParaNet also produces stable alignment between text and speech on
  the challenging test sentences by iteratively improving the attention in a layer-by-layer
  manner. Furthermore, we build the parallel text-to-speech system by applying various
  parallel neural vocoders, which can synthesize speech from text through a single
  feed-forward pass. We also explore a novel VAE-based approach to train the inverse
  autoregressive flowÂ (IAF) based parallel vocoder from scratch, which avoids the
  need for distillation from a separately trained WaveNet as previous work.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: peng20a
month: 0
tex_title: Non-Autoregressive Neural Text-to-Speech
firstpage: 7586
lastpage: 7598
page: 7586-7598
order: 7586
cycles: false
bibtex_author: Peng, Kainan and Ping, Wei and Song, Zhao and Zhao, Kexin
author:
- given: Kainan
  family: Peng
- given: Wei
  family: Ping
- given: Zhao
  family: Song
- given: Kexin
  family: Zhao
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
