---
title: Sparse Sinkhorn Attention
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/tay20a/tay20a.pdf
url: http://proceedings.mlr.press/v119/tay20a.html
abstract: We propose Sparse Sinkhorn Attention, a new efficient and sparse method
  for learning to attend. Our method is based on differentiable sorting of internal
  representations. Concretely, we introduce a meta sorting network that learns to
  generate latent permutations over sequences. Given sorted sequences, we are then
  able to compute quasi-global attention with only local windows, improving the memory
  efficiency of the attention module. To this end, we propose new algorithmic innovations
  such as Causal Sinkhorn Balancing and SortCut, a dynamic sequence truncation method
  for tailoring Sinkhorn Attention for encoding and/or decoding purposes. Via extensive
  experiments on algorithmic seq2seq sorting, language modeling, pixel-wise image
  generation, document classification and natural language inference, we demonstrate
  that our memory efficient Sinkhorn Attention method is competitive with vanilla
  attention and consistently outperforms recently proposed efficient Transformer models
  such as Sparse Transformers.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tay20a
month: 0
tex_title: Sparse {S}inkhorn Attention
firstpage: 9438
lastpage: 9447
page: 9438-9447
order: 9438
cycles: false
bibtex_author: Tay, Yi and Bahri, Dara and Yang, Liu and Metzler, Donald and Juan,
  Da-Cheng
author:
- given: Yi
  family: Tay
- given: Dara
  family: Bahri
- given: Liu
  family: Yang
- given: Donald
  family: Metzler
- given: Da-Cheng
  family: Juan
date: 2020-09-29
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 9
  - 29
extras: []
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
