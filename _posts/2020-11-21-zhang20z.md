---
title: Attacks Which Do Not Kill Training Make Adversarial Learning Stronger
booktitle: Proceedings of the 37th International Conference on Machine Learning
year: '2020'
pdf: http://proceedings.mlr.press/v119/zhang20z/zhang20z.pdf
url: http://proceedings.mlr.press/v119/zhang20z.html
abstract: 'Adversarial training based on the minimax formulation is necessary for
  obtaining adversarial robustness of trained models. However, it is conservative
  or even pessimistic so that it sometimes hurts the natural generalization. In this
  paper, we raise a fundamental question{—}do we have to trade off natural generalization
  for adversarial robustness? We argue that adversarial training is to employ confident
  adversarial data for updating the current model. We propose a novel formulation
  of friendly adversarial training (FAT): rather than employing most adversarial data
  maximizing the loss, we search for least adversarial data (i.e., friendly adversarial
  data) minimizing the loss, among the adversarial data that are confidently misclassified.
  Our novel formulation is easy to implement by just stopping the most adversarial
  data searching algorithms such as PGD (projected gradient descent) early, which
  we call early-stopped PGD. Theoretically, FAT is justified by an upper bound of
  the adversarial risk. Empirically, early-stopped PGD allows us to answer the earlier
  question negatively{—}adversarial robustness can indeed be achieved without compromising
  the natural generalization.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhang20z
month: 0
tex_title: Attacks Which Do Not Kill Training Make Adversarial Learning Stronger
firstpage: 11278
lastpage: 11287
page: 11278-11287
order: 11278
cycles: false
bibtex_author: Zhang, Jingfeng and Xu, Xilie and Han, Bo and Niu, Gang and Cui, Lizhen
  and Sugiyama, Masashi and Kankanhalli, Mohan
author:
- given: Jingfeng
  family: Zhang
- given: Xilie
  family: Xu
- given: Bo
  family: Han
- given: Gang
  family: Niu
- given: Lizhen
  family: Cui
- given: Masashi
  family: Sugiyama
- given: Mohan
  family: Kankanhalli
date: 2020-11-21
address: 
container-title: Proceedings of the 37th International Conference on Machine Learning
volume: '119'
genre: inproceedings
issued:
  date-parts:
  - 2020
  - 11
  - 21
software: https://github.com/zjfheart/Friendly-Adversarial-Training
extras:
- label: Supplementary PDF
  link: http://proceedings.mlr.press/v119/zhang20z/zhang20z-supp.pdf
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
